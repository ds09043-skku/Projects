{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b72MthKykO4",
        "outputId": "076e879b-de30-4ec3-be91-3245822fc681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Task 1: Implementing Neural Networks\n",
            "--------------------------------------------------\n",
            "\n",
            "PyTorch Result:\n",
            "Input x1 = [1. 2. 3.]'s Output: [0.92414182 0.07585818]\n",
            "Input x2 = [4. 5. 6.]'s Output: [0.99649481 0.00350519]\n",
            "\n",
            "Numpy Result:\n",
            "\n",
            "Input x1 = [1. 2. 3.]'s Output: [0.92414182 0.07585818]\n",
            "Input x2 = [4. 5. 6.]'s Output: [0.99649481 0.00350519]\n",
            "\n",
            "Task 2: Loss function and Computing Gradients\n",
            "--------------------------------------------------\n",
            "\n",
            "PyTorch Result:\n",
            "Loss for x1: 2.578890\n",
            "w1 gradient for input x1:\n",
            "[[ 0.09241418 -0.09241418  0.36965673  0.09241418]\n",
            " [ 0.18482836 -0.18482836  0.73931346  0.18482836]\n",
            " [ 0.27724255 -0.27724255  1.10897018  0.27724255]]\n",
            "Loss for x2: 0.003511\n",
            "w1 gradient for input x2:\n",
            "[[-0.00140207  0.00140207 -0.0056083  -0.00140207]\n",
            " [-0.00175259  0.00175259 -0.00701037 -0.00175259]\n",
            " [-0.00210311  0.00210311 -0.00841245 -0.00210311]]\n",
            "Total Loss: 2.582401\n",
            "Total gradient with respect to w1:\n",
            "[[ 0.09101211 -0.09101211  0.36404843  0.09101211]\n",
            " [ 0.18307577 -0.18307577  0.73230308  0.18307577]\n",
            " [ 0.27513943 -0.27513943  1.10055773  0.27513943]]\n",
            "\n",
            "Numpy Result:\n",
            "Loss for x1: 2.578890\n",
            "w1 gradient for input x1:\n",
            "[[ 0.09241418 -0.09241418  0.36965673  0.09241418]\n",
            " [ 0.18482836 -0.18482836  0.73931346  0.18482836]\n",
            " [ 0.27724255 -0.27724255  1.10897018  0.27724255]]\n",
            "Loss for x2: 0.003511\n",
            "w1 gradient for input x2:\n",
            "[[-0.00140207  0.00140207 -0.0056083  -0.00140207]\n",
            " [-0.00175259  0.00175259 -0.00701037 -0.00175259]\n",
            " [-0.00210311  0.00210311 -0.00841245 -0.00210311]]\n",
            "Total Loss: 2.582401\n",
            "Total gradient with respect to w1:\n",
            "[[ 0.09101211 -0.09101211  0.36404843  0.09101211]\n",
            " [ 0.18307577 -0.18307577  0.73230308  0.18307577]\n",
            " [ 0.27513943 -0.27513943  1.10055773  0.27513943]]\n",
            "\n",
            "Task 3: 100 times iteration and updating weights\n",
            "--------------------------------------------------\n",
            "\n",
            "w1 after iteration:\n",
            "PyTorch: \n",
            "[[0.06286026 0.0757553  0.42864883 0.39297265]\n",
            " [0.4759097  0.55350224 0.71103825 0.7772103 ]\n",
            " [0.88895913 1.03124918 0.99342767 1.16144794]]\n",
            "Numpy: \n",
            "[[0.06286026 0.0757553  0.42864883 0.39297265]\n",
            " [0.4759097  0.55350224 0.71103825 0.7772103 ]\n",
            " [0.88895913 1.03124918 0.99342767 1.16144794]]\n",
            "\n",
            "w2 after iteration:\n",
            "PyTorch: \n",
            "[[0.07667678 0.22332322]\n",
            " [0.28480592 0.61519408]\n",
            " [0.57952531 0.22047469]\n",
            " [0.77528441 0.72471559]]\n",
            "Numpy: \n",
            "[[0.07667678 0.22332322]\n",
            " [0.28480592 0.61519408]\n",
            " [0.57952531 0.22047469]\n",
            " [0.77528441 0.72471559]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Seed (Actually, it's not needed since there is no randomness in the code)\n",
        "np.random.seed(2021313549)\n",
        "torch.manual_seed(2021313549)\n",
        "\n",
        "# input\n",
        "x1 = np.array([1.0, 2.0, 3.0])\n",
        "x2 = np.array([4.0, 5.0, 6.0])\n",
        "\n",
        "# weight\n",
        "w1 = np.array([[0.1, 0.2, 0.3, 0.4],\n",
        "               [0.5, 0.6, 0.7, 0.8],\n",
        "               [0.9, 1.0, 1.1, 1.2]])\n",
        "\n",
        "w2 = np.array([[0.2, 0.1],\n",
        "               [0.4, 0.5],\n",
        "               [0.6, 0.2],\n",
        "               [0.8, 0.7]])\n",
        "\n",
        "# target\n",
        "y1_target = np.array([0, 1])\n",
        "y2_target = np.array([1, 0])\n",
        "\n",
        "# numpy to torch\n",
        "x1_tensor = torch.from_numpy(x1)\n",
        "x2_tensor = torch.from_numpy(x2)\n",
        "w1_tensor = torch.from_numpy(w1)\n",
        "w1_tensor.requires_grad_(True)    # for tracking gradients\n",
        "w2_tensor = torch.from_numpy(w2)\n",
        "w2_tensor.requires_grad_(True)\n",
        "y1_target_tensor = torch.from_numpy(y1_target)\n",
        "y2_target_tensor = torch.from_numpy(y2_target)\n",
        "\n",
        "# <Task 1>\n",
        "print(\"\\nTask 1: Implementing Neural Networks\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# forward function for torch\n",
        "def torch_forward(x, w1, w2):\n",
        "    # input -> hidden\n",
        "    net1 = torch.matmul(x, w1)    # Net input of ReLU\n",
        "    h = torch.relu(net1)    # Output of ReLU\n",
        "\n",
        "    # hidden -> output\n",
        "    net2 = torch.matmul(h, w2)    # Net input of softmax\n",
        "    o = torch.softmax(net2, dim=0)    # Output of softmax\n",
        "\n",
        "    return o, h, net1, net2\n",
        "\n",
        "# forward function for numpy\n",
        "def numpy_forward(x, w1, w2):\n",
        "    # input -> hidden\n",
        "    net1 = np.dot(x, w1)\n",
        "    h = np.maximum(0, net1)    # ReLU\n",
        "\n",
        "    # hidden -> output\n",
        "    net2 = np.dot(h, w2)\n",
        "    o = np.exp(net2) / np.sum(np.exp(net2))    # softmax\n",
        "\n",
        "    return o, h, net1, net2\n",
        "\n",
        "# forward for torch\n",
        "y1_tensor, h_tensor_x1, net1_tensor_x1, net2_tensor_x1 = torch_forward(x1_tensor, w1_tensor, w2_tensor)\n",
        "y2_tensor, h_tensor_x2, net1_tensor_x2, net2_tensor_x2 = torch_forward(x2_tensor, w1_tensor, w2_tensor)\n",
        "\n",
        "# forward for numpy\n",
        "y1_numpy, h_numpy_x1, net1_numpy_x1, net2_numpy_x1 = numpy_forward(x1, w1, w2)\n",
        "y2_numpy, h_numpy_x2, net1_numpy_x2, net2_numpy_x2 = numpy_forward(x2, w1, w2)\n",
        "\n",
        "print(\"\\nPyTorch Result:\")\n",
        "print(f\"Input x1 = {x1_tensor.numpy()}'s Output: {y1_tensor.detach().numpy()}\")    # detach() for no gradient\n",
        "print(f\"Input x2 = {x2_tensor.numpy()}'s Output: {y2_tensor.detach().numpy()}\")\n",
        "\n",
        "print(\"\\nNumpy Result:\")\n",
        "print(f\"Input x1 = {x1}'s Output: {y1_numpy}\")\n",
        "print(f\"Input x2 = {x2}'s Output: {y2_numpy}\")\n",
        "\n",
        "# <Task 2>\n",
        "print(\"\\nTask 2: Loss function and Computing Gradients\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# cross entropy loss for torch\n",
        "def cross_entropy_loss_torch(y_pred, y_true):\n",
        "    return -torch.sum(y_true * torch.log(y_pred))\n",
        "\n",
        "# cross entropy loss for numpy\n",
        "def cross_entropy_loss_numpy(y_pred, y_true):\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "# backward for numpy\n",
        "def backward_numpy(x, y_pred, y_true, h, w2):\n",
        "    do = y_pred - y_true    # output layer error\n",
        "\n",
        "    grad_w2 = np.outer(h, do)    # w2 gradient\n",
        "\n",
        "    dh = np.dot(do, w2.T)    # hidden layer error\n",
        "    dh *= (h > 0)    # ReLU derivative: h'(x) = 0 if x <= 0, 1 if x > 0\n",
        "\n",
        "    grad_w1 = np.outer(x, dh)    # w1 gradient\n",
        "\n",
        "    return grad_w1, grad_w2\n",
        "\n",
        "# x1 y1 torch\n",
        "loss_x1_tensor = cross_entropy_loss_torch(y1_tensor, y1_target_tensor)\n",
        "loss_x1_tensor.backward(retain_graph=True)\n",
        "dw1_x1_tensor = w1_tensor.grad.clone()    # clone() : avoid messing up original tensor (in-place operation)\n",
        "dw2_x1_tensor = w2_tensor.grad.clone()\n",
        "w1_tensor.grad.zero_()    # zero_() : reset gradient\n",
        "w2_tensor.grad.zero_()\n",
        "\n",
        "# x1 y1 numpy\n",
        "loss_x1_numpy = cross_entropy_loss_numpy(y1_numpy, y1_target)\n",
        "dw1_x1_numpy, dw2_x1_numpy = backward_numpy(x1, y1_numpy, y1_target, h_numpy_x1, w2)\n",
        "\n",
        "# x2 y2 torch\n",
        "loss_x2_tensor = cross_entropy_loss_torch(y2_tensor, y2_target_tensor)\n",
        "loss_x2_tensor.backward(retain_graph=True)\n",
        "dw1_x2_tensor = w1_tensor.grad.clone()\n",
        "dw2_x2_tensor = w2_tensor.grad.clone()\n",
        "w1_tensor.grad.zero_()\n",
        "w2_tensor.grad.zero_()\n",
        "\n",
        "# x2 y2 numpy\n",
        "loss_x2_numpy = cross_entropy_loss_numpy(y2_numpy, y2_target)\n",
        "dw1_x2_numpy, dw2_x2_numpy = backward_numpy(x2, y2_numpy, y2_target, h_numpy_x2, w2)\n",
        "\n",
        "# result\n",
        "print(\"\\nPyTorch Result:\")\n",
        "print(f\"Loss for x1: {loss_x1_tensor.item():.6f}\")    # item() : convert tensor to python float\n",
        "print(f\"w1 gradient for input x1:\\n{dw1_x1_tensor.detach().numpy()}\")\n",
        "print(f\"Loss for x2: {loss_x2_tensor.item():.6f}\")\n",
        "print(f\"w1 gradient for input x2:\\n{dw1_x2_tensor.detach().numpy()}\")\n",
        "print(f\"Total Loss: {loss_x1_tensor.item() + loss_x2_tensor.item():.6f}\")\n",
        "print(f\"Total gradient with respect to w1:\\n{dw1_x1_tensor.detach().numpy() + dw1_x2_tensor.detach().numpy()}\")\n",
        "\n",
        "print(\"\\nNumpy Result:\")\n",
        "print(f\"Loss for x1: {loss_x1_numpy:.6f}\")\n",
        "print(f\"w1 gradient for input x1:\\n{dw1_x1_numpy}\")\n",
        "print(f\"Loss for x2: {loss_x2_numpy:.6f}\")\n",
        "print(f\"w1 gradient for input x2:\\n{dw1_x2_numpy}\")\n",
        "print(f\"Total Loss: {loss_x1_numpy + loss_x2_numpy:.6f}\")\n",
        "print(f\"Total gradient with respect to w1:\\n{dw1_x1_numpy + dw1_x2_numpy}\")\n",
        "\n",
        "# <Task 3>\n",
        "print(\"\\nTask 3: 100 times iteration and updating weights\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# hyper parameter\n",
        "learning_rate = 0.01\n",
        "num_epochs = 100\n",
        "\n",
        "# training\n",
        "def train_torch():\n",
        "    # initialize weights\n",
        "    w1_train = w1_tensor.clone().detach().requires_grad_(True)\n",
        "    w2_train = w2_tensor.clone().detach().requires_grad_(True)\n",
        "    # clone() : clone the tensor\n",
        "    # detach() : seperate new(cloned) tensor from the original tensor\n",
        "    # requires_grad_(True) : enable gradient tracking again (for cloned tensor)\n",
        "    # reason for all these: shallow copy of original tensor\n",
        "\n",
        "    losses = []    # store total loss for each epoch\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # forward with x1\n",
        "        y1_pred, _, _, _ = torch_forward(x1_tensor, w1_train, w2_train)    # _ : not used (only want y1_pred)\n",
        "        loss1 = cross_entropy_loss_torch(y1_pred, y1_target_tensor)\n",
        "\n",
        "        y2_pred, _, _, _ = torch_forward(x2_tensor, w1_train, w2_train)\n",
        "        loss2 = cross_entropy_loss_torch(y2_pred, y2_target_tensor)\n",
        "\n",
        "        # total loss\n",
        "        total_loss = loss1 + loss2\n",
        "        losses.append(total_loss.item())\n",
        "\n",
        "        # backward\n",
        "        total_loss.backward()\n",
        "\n",
        "        # update weights\n",
        "        with torch.no_grad():    # no_grad() : pause gradient tracking inside the block\n",
        "            # without no_grad(), next backward() will also compute this operation\n",
        "            w1_train -= learning_rate * w1_train.grad\n",
        "            w2_train -= learning_rate * w2_train.grad\n",
        "\n",
        "        # initialize gradients, without this, backward() will compute gradients of previous epoch\n",
        "        w1_train.grad.zero_()\n",
        "        w2_train.grad.zero_()\n",
        "\n",
        "    return w1_train.detach().numpy(), w2_train.detach().numpy(), losses\n",
        "\n",
        "def train_numpy():\n",
        "    w1_train = w1.copy()    # copy() : shallow copy for numpy\n",
        "    w2_train = w2.copy()\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # forward with x1\n",
        "        y1_pred, h_x1, _, _ = numpy_forward(x1, w1_train, w2_train)\n",
        "        loss1 = cross_entropy_loss_numpy(y1_pred, y1_target)\n",
        "\n",
        "        y2_pred, h_x2, _, _ = numpy_forward(x2, w1_train, w2_train)\n",
        "        loss2 = cross_entropy_loss_numpy(y2_pred, y2_target)\n",
        "\n",
        "        # total loss\n",
        "        total_loss = loss1 + loss2\n",
        "        losses.append(total_loss)\n",
        "\n",
        "        # backward\n",
        "        dw1_x1, dw2_x1 = backward_numpy(x1, y1_pred, y1_target, h_x1, w2_train)\n",
        "        dw1_x2, dw2_x2 = backward_numpy(x2, y2_pred, y2_target, h_x2, w2_train)\n",
        "        dw1 = dw1_x1 + dw1_x2    # total gradient for all inputs\n",
        "        dw2 = dw2_x1 + dw2_x2\n",
        "\n",
        "        # update weights\n",
        "        w1_train -= learning_rate * dw1\n",
        "        w2_train -= learning_rate * dw2\n",
        "\n",
        "    return w1_train, w2_train, losses\n",
        "\n",
        "# training\n",
        "w1_final_torch, w2_final_torch, losses_torch = train_torch()\n",
        "w1_final_numpy, w2_final_numpy, losses_numpy = train_numpy()\n",
        "\n",
        "# final result\n",
        "print(\"\\nw1 after iteration:\")\n",
        "print(f\"PyTorch: \\n{w1_final_torch}\")\n",
        "print(f\"Numpy: \\n{w1_final_numpy}\")\n",
        "print(\"\\nw2 after iteration:\")\n",
        "print(f\"PyTorch: \\n{w2_final_torch}\")\n",
        "print(f\"Numpy: \\n{w2_final_numpy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
