{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install openai"
      ],
      "metadata": {
        "id": "49OBSFQ3k39h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --quiet"
      ],
      "metadata": {
        "id": "Dx6wYawqhwgz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improt necessary libraries"
      ],
      "metadata": {
        "id": "lRpzDu9hkkxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score"
      ],
      "metadata": {
        "id": "p59DyHJvhqqj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type-in your given openai api key"
      ],
      "metadata": {
        "id": "nF64HWqSk6hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=\"\")"
      ],
      "metadata": {
        "id": "cbGmWgn9h8Wu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load test.csv file"
      ],
      "metadata": {
        "id": "C2PimyuflAwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "INy6TKcDh-FO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_content = ''\n",
        "for idx, row in df.iterrows():\n",
        "    entry_id = row['ID']\n",
        "    title = str(row['title']).strip()\n",
        "    notes = str(row['notes']).strip()\n",
        "    content = f\"{entry_id}: {title} {notes}\"\n",
        "    user_content += content + '\\n'"
      ],
      "metadata": {
        "id": "xFXbsmEUidn1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All entries from train.csv\n",
        "train_df = pd.read_csv('train.csv')\n",
        "examples_all = ''\n",
        "for idx, row in train_df.iterrows():\n",
        "    entry_id = row['ID']\n",
        "    title = str(row['title']).strip()\n",
        "    notes = str(row['notes']).strip()\n",
        "    target = row['target']\n",
        "    content = f\"ID: {entry_id}\\nTitle: {title}\\nNotes: {notes}\\nLabel: {target}\"\n",
        "    examples_all += content + '\\n\\n'\n",
        "#print(examples_all)"
      ],
      "metadata": {
        "id": "nerqzpBLiaOf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually selecting train examples (old version)\n",
        "old_examples_selected = \"\"\"\n",
        "ID: TRAIN_00\n",
        "Title: Steuerbarer Umsatz aus Lieferungen und Leistungen in Flensburg\n",
        "Notes: Steuern, Finanzen, Öffentlicher Dienst - Steuern - Umsatzsteuerstatistik (Voranmeldungen) - Steuerbarer Umsatz aus Lieferungen und Leistungen in Flensburg\n",
        "Zum HTML-Angebot der Zeitreihe\n",
        "Regionaldaten für Schleswig-Holstein\n",
        "Statistisches Amt für Hamburg und Schleswig-Holstein\n",
        "Label: 0\n",
        "\n",
        "ID: TRAIN_01\n",
        "Title: 도로교통공단_고속도로구간별 도로위험도지수정보 조회 서비스\n",
        "Notes: 고속도로명, 고속도로구간명과 차종코드를 이용하여 구간 내의 도로위험도지수값, 도로위험도등급 등의 구간좌표별 실시간 도로위험도지수정보를 제공하는 서비스\n",
        "Label: 1\n",
        "\n",
        "ID: TRAIN_02\n",
        "Title: New registrations of road vehicles by vehicle group and type\n",
        "Notes: This dataset presents the monthly data of new registrations of road vehicles (passenger cars, passenger vehicles, goods vehicles, agricultural vehicles, industrial vehicles, motorcycles, trailers) by canton and fuel, since 2005.\n",
        "Label: 1\n",
        "\n",
        "ID: TRAIN_09\n",
        "Title: 경기도 양평군_고령인구현황\n",
        "Notes: 2022년 양평군 노인들을 대상으로 노인복지 사각복지를 파악하기 위해서 설문조사를 실시해서 얻은 데이터\n",
        "설문에 응답하지 않는 문항의 값은 -1\n",
        "Label: 0\n",
        "\n",
        "ID: TRAIN_10\n",
        "Title: Tasas de actividad, empleo y paro por tipo de municipio. (A partir de 2017) (Identificador API: /t22/p320/base_2015/serie/l0/03003.px)\n",
        "Notes: Tasas de actividad, empleo y paro por tipo de municipio. (A partir de 2017). Nacional. Estadística del Empleo de las Personas con Discapacidad\n",
        "Label: 0\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5_qCGmVuZkaQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually selecting train examples (new version)\n",
        "examples_selected=\"\"\"\n",
        "ID: TRAIN_01\n",
        "Title: 도로교통공단_고속도로구간별 도로위험도지수정보 조회 서비스\n",
        "Notes: 고속도로명, 고속도로구간명과 차종코드를 이용하여 구간 내의 도로위험도지수값, 도로위험도등급 등의 구간좌표별 실시간 도로위험도지수정보를 제공하는 서비스\n",
        "Label: 1\n",
        "\n",
        "ID: TRAIN_02\n",
        "Title: New registrations of road vehicles by vehicle group and type\n",
        "Notes: This dataset presents the monthly data of new registrations of road vehicles (passenger cars, passenger vehicles, goods vehicles, agricultural vehicles, industrial vehicles, motorcycles, trailers) by canton and fuel, since 2005.\n",
        "Label: 1\n",
        "\n",
        "ID: TRAIN_04\n",
        "Title: Marine Geophysical and Seismic Data from around the UK (1966 Onwards)\n",
        "Notes: The British Geological Survey hold a collection of data recorded during marine geophysical surveys which includes digital data and analogue records. These data result from approximately 350,000 line kilometres of multi-instrument geophysical survey lines. The data include seismic, sonar, magnetic, gravity, echo sounder, multibeam bathymetry and navigation data. The seismic data are mainly for airgun, sparker, boomer and pinger. The data were primarily collected by BGS and the collection also includes additional third party data. The data are primarily from the UKCS (United Kingdom Continental Shelf). The data are stored within the National Geoscience Data Centre (NGDC) and the Marine Environmental Data and Information Network (MEDIN) Data Archive Centre (DAC) for Geology and Geophysics. The majority of legacy geophysical paper records are available as scanned images viewable via the BGS Offshore GeoIndex www.bgs.ac.uk/GeoIndex/offshore.htm. Other records can be scanned on request. Older records are of variable quality. Data not yet available online including segy are available on request enquiries@bgs.ac.uk. The data are applicable to a wide range of uses including environmental, geotechnical, geophysical and geological studies. Reference: Fannin, NGT. (1989) Offshore Investigations 1966-87. British Geological Survey Technical Report WB/89/2, British Geological Survey.\n",
        "Label: 0\n",
        "\n",
        "ID: TRAIN_05\n",
        "Title: Truck Size and Weight Enforcement Data\n",
        "Notes: This dataset consists of truck size and weight enforcement data including number of trucks weighed, number of violations, and number of oversize/overweight permits, as reported by the States in their annual certification to FHWA.\n",
        "Label: 1\n",
        "\n",
        "ID: TRAIN_10\n",
        "Title: Tasas de actividad, empleo y paro por tipo de municipio. (A partir de 2017) (Identificador API: /t22/p320/base_2015/serie/l0/03003.px)\n",
        "Notes: Tasas de actividad, empleo y paro por tipo de municipio. (A partir de 2017). Nacional. Estadística del Empleo de las Personas con Discapacidad\n",
        "Label: 0\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ep5yGXlTw6Da"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train examples seperated by label\n",
        "train_df = pd.read_csv('train.csv')\n",
        "examples_0 = ''\n",
        "examples_1 = ''\n",
        "for idx, row in train_df.iterrows():\n",
        "    entry_id = row['ID']\n",
        "    title = str(row['title']).strip()\n",
        "    notes = str(row['notes']).strip()\n",
        "    target = row['target']\n",
        "    content = f\"ID: {entry_id}\\nTitle: {title}\\nNotes: {notes}\\nLabel: {target}\"\n",
        "\n",
        "    if target == 0:\n",
        "        examples_0 += content + '\\n\\n'\n",
        "    elif target == 1:\n",
        "        examples_1 += content + '\\n\\n'\n",
        "\n",
        "#print(examples_0)\n",
        "#print(examples_1)"
      ],
      "metadata": {
        "id": "ClPiiu3j1ddl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples sampled from each language\n",
        "train_df = pd.read_csv('train.csv')\n",
        "examples_per_lang = 2  #2 examples per language\n",
        "examples_lang = ''\n",
        "grouped = train_df.groupby('lang')\n",
        "for lang, group in grouped:\n",
        "    sampled = group.sample(n=min(len(group), examples_per_lang), random_state=42)\n",
        "    for _, row in sampled.iterrows():\n",
        "        entry_id = row['ID']\n",
        "        title = str(row['title']).strip()\n",
        "        notes = str(row['notes']).strip()\n",
        "        target = row['target']\n",
        "        content = f\"ID: {entry_id}\\nTitle: {title}\\nNotes: {notes}\\nLabel: {target}\"\n",
        "        examples_lang += content + '\\n\\n'\n",
        "#print(examples_lang)"
      ],
      "metadata": {
        "id": "X79SfUX-fexZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original given system message\n",
        "given_system_message=\"You are an automotive data expert. For each incoming data entry, determine whether the data is automotive-related (1) or not (0). There are 40 data entries labeled from ID: TEST_00 to TEST_39. Respond with 0 or 1 for each entry, separated by rows. If the output does not contain exactly 40 rows, the power will be shut off.\""
      ],
      "metadata": {
        "id": "P56FTFCNhojD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once highest prompt (h:0.825),(avg:0.7). But p(err) is too high\n",
        "old_system_message = f\"\"\"\n",
        "You are an expert in classifying data entries as automotive-related (1) or not automotive-related (0).\n",
        "\n",
        "**Task:**\n",
        "Classify whether a data entry is automotive-related based on its title and notes. Follow these rules:\n",
        "1. Automotive-related entries often mention vehicles, transportation systems, or traffic data.\n",
        "2. Non-automotive entries usually describe general topics like taxation, geography, or unrelated statistics.\n",
        "\n",
        "**Examples:**\n",
        "{examples_all}\n",
        "\n",
        "**Instructions:**\n",
        "- Output \"1\" for automotive-related entries and \"0\" for others.\n",
        "- Respond with exactly 40 predictions (one per line) for the following entries. Any deviation will be flagged as an error.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wztCtfjZ_gs9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# averagely sound version (avg:0.725)\n",
        "system_message = f\"\"\"\n",
        "You are an expert in classifying data entries as automotive-related (1) or not automotive-related (0).\n",
        "\n",
        "###Task\n",
        "- Classify each data entry as either \"1\" (automotive-related) or \"0\" (not automotive-related).\n",
        "- You must output exactly 40 lines, with one prediction (0 or 1) on each line.\n",
        "\n",
        "###Examples\n",
        "- Here are some examples of labeled data entries:\n",
        "{old_examples_selected}\n",
        "\n",
        "###Instructions\n",
        "1. Classify the following 40 data entries labeled from ID: TEST_00 to TEST_39.\n",
        "2. Consider the title, notes, and the language of each entry when making the classification.\n",
        "3. Automotive-related entries often mention roads, vehicles, transportation systems, or traffic data in the title or notes using their own language.\n",
        "4. Non-automotive entries usually describe other general topics like taxation, geography, or unrelated statistics.\n",
        "\n",
        "**IMPORTANT**\n",
        "- Your output MUST have exactly 40 lines.\n",
        "- Each line must contain only \"0\" or \"1\". No additional text is allowed.\n",
        "- Any deviation especially about the number of output lines will result in penalties.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "g5aMRN6ohSi1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling method -> (avg:0.675) -> not good\n",
        "system_message_sampling = f\"\"\"\n",
        "You are an expert in classifying data entries as automotive-related (1) or not automotive-related (0).\n",
        "\n",
        "### Task\n",
        "1. Classify each data entry as either \"1\" (automotive-related) or \"0\" (not automotive-related).\n",
        "2. Perform this classification 10 times independently for each entry.\n",
        "3. Output the most frequent classification result (majority vote) for each entry as the final prediction.\n",
        "\n",
        "### Examples\n",
        "Here are some examples of labeled data:\n",
        "{examples_all}\n",
        "\n",
        "### Instructions\n",
        "1. Classify the following 40 data entries labeled from ID: TEST_00 to TEST_39.\n",
        "2. If you determine that there are more than or less than 40 entries, check again until there are exactly 40 entries.\n",
        "3. Repeat the classification process **10 times for each entry**.\n",
        "4. Determine the final classification for each entry by taking the majority vote among the 10 results.\n",
        "5. Your final output must contain exactly 40 lines, one prediction per line (0 or 1).\n",
        "\n",
        "### Important Rules\n",
        "- Each line should contain only \"0\" or \"1\".\n",
        "- If the output does not contain exactly 40 lines, the power will be shut off and penalties will apply.\n",
        "- You must ensure that the most frequent classification (majority vote) is used for each entry as the final result.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9-e1mvik2W_H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this prompt generates semi-refined response.\n",
        "semirefined_system_message = f\"\"\"\n",
        "You are an expert in classifying data entries as automotive-related (1) or not automotive-related (0).\n",
        "\n",
        "###Task\n",
        "- Classify each data entry as either \"1\" (automotive-related) or \"0\" (not automotive-related).\n",
        "- You must output exactly 40 lines, with one prediction (0 or 1) on each line.\n",
        "\n",
        "###Examples\n",
        "- Here are some examples of labeled data entries:\n",
        "{examples_all}\n",
        "\n",
        "###Instructions\n",
        "1. Classify the following 40 data entries labeled from ID: TEST_00 to TEST_39.\n",
        "2. Consider the title, notes, and the language of each entry when making the classification.\n",
        "3. Automotive-related entries often mention roads, vehicles, transportation systems, or traffic data in the title or notes using their own language.\n",
        "4. Non-automotive entries usually describe other general topics like taxation, geography, or unrelated statistics.\n",
        "\n",
        "**IMPORTANT**\n",
        "- Your output MUST have exactly 40 lines.\n",
        "- Respond format:\n",
        "TEST_00: 1\n",
        "TEST_01: 0\n",
        "TEST_02: 0\n",
        "TEST_03: 1\n",
        "...\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YEs7jDjgc9UA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the content section below to enhance classification"
      ],
      "metadata": {
        "id": "9XzCQE7elFgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "92ciLzCeQ1GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ad1c20-4862-4d5b-d160-b0958811f11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total token used: 6771\n"
          ]
        }
      ],
      "source": [
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0125\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": semirefined_system_message},\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ],\n",
        "    temperature=0.4\n",
        ")\n",
        "\n",
        "response = completion.choices[0].message.content\n",
        "#print(response)\n",
        "n_token = completion.usage.total_tokens\n",
        "print(f\"\\nTotal token used: {n_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse the assistant's response to get the predicted labels"
      ],
      "metadata": {
        "id": "pYdz8syTlUa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = response.strip().split('\\n')\n",
        "lines = [line.split(\": \")[1] for line in lines if \": \" in line]"
      ],
      "metadata": {
        "id": "exZDR5BolTGD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation part"
      ],
      "metadata": {
        "id": "tteP2bRdljZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if we have 40 predictions\n",
        "if len(lines) != 40:\n",
        "    print(f\"Error: The assistant did not return 40 predictions. It returned {len(lines)} lines.\")\n",
        "else:\n",
        "    predicted_labels = []\n",
        "    for line in lines:\n",
        "        # Extract the prediction (0 or 1) from each line\n",
        "        match = re.search(r'\\b(0|1)\\b', line)\n",
        "        if match:\n",
        "            pred = match.group(1)\n",
        "            predicted_labels.append(int(pred))\n",
        "        else:\n",
        "            print(f\"Could not extract prediction from line: {line}\")\n",
        "            predicted_labels.append(None)\n",
        "    # Actual target values from the CSV\n",
        "    actual_labels = df['target'].tolist()\n",
        "\n",
        "    # Remove any None values from predictions\n",
        "    valid_indices = [i for i, x in enumerate(predicted_labels) if x is not None]\n",
        "    predicted_labels_valid = [predicted_labels[i] for i in valid_indices]\n",
        "    actual_labels_valid = [actual_labels[i] for i in valid_indices]\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(actual_labels_valid, predicted_labels_valid)\n",
        "    macro_f1 = f1_score(actual_labels_valid, predicted_labels_valid, average='macro')\n",
        "    print(f\"\\nAccuracy: {accuracy}\")\n",
        "    print(f\"Macro F1 Score: {macro_f1}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(actual_labels_valid, predicted_labels_valid))"
      ],
      "metadata": {
        "id": "od8k3wDDjNZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33792487-8909-4879-e8a6-5a607e2956a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.925\n",
            "Macro F1 Score: 0.9249530956848031\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.93        20\n",
            "           1       0.95      0.90      0.92        20\n",
            "\n",
            "    accuracy                           0.93        40\n",
            "   macro avg       0.93      0.93      0.92        40\n",
            "weighted avg       0.93      0.93      0.92        40\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "프롬프트 엔지니어링(단계별 지시):\n",
        "https://www.aiground.co.kr/mastering-chatgpt-prompt-engineering/\n",
        "\n",
        "프롬프트 엔지니어링(협박, 어순 등):\n",
        "https://kr.linkedin.com/posts/jocoding_%EC%B1%97gpt%EC%97%90%EA%B2%8C-%ED%8C%81%EC%9D%84-%EC%A4%80%EB%8B%A4%EA%B3%A0-%ED%95%98%EA%B1%B0%EB%82%98-%ED%98%91%EB%B0%95%ED%95%98%EB%A9%B4-%EB%8D%94-%EC%A2%8B%EC%9D%80-%EB%8B%B5%EB%B3%80%EC%9D%84-%EC%A4%80%EB%8B%A4%EB%8A%94-%EC%82%AC%EC%8B%A4-%EC%95%8C%EA%B3%A0-activity-7167405031309209600-Koni\n",
        "\n",
        "https://www.youtube.com/watch?v=mC2b57u_s0k\n",
        "\n",
        "GPT를 사용한 분류 모델:\n",
        "https://medium.com/@hugmanskj/hands-on-%EA%B1%B0%EB%8C%80%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EA%B8%B0%EB%B0%98-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B6%84%EB%A5%98-6e9537243eec"
      ],
      "metadata": {
        "id": "4AqLx5SZoDOZ"
      }
    }
  ]
}